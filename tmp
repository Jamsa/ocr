
#定义CNN网络，处理图片，
def convolutional_layers1():
    #输入数据，shape [batch_size, 32, 256, 3]
    inputs = tf.placeholder(tf.float32, [BATCH_SIZE, OUTPUT_SHAPE[0], OUTPUT_SHAPE[1], 3])    

    #第一层卷积层, 32*256*3 => 16*128*48
    W_conv1 = weight_variable([5, 5, 3, 48])
    b_conv1 = bias_variable([48])
    #x_expanded = tf.expand_dims(inputs, 3)
    x_expanded = inputs
    h_conv1 = tf.nn.relu(conv2d(x_expanded, W_conv1) + b_conv1)
    h_pool1 = max_pool(h_conv1, ksize=(2, 2), stride=(2, 2))

    #return inputs, h_pool1
    #第二层, 16*128*48 => 16*64*64
    W_conv2 = weight_variable([5, 5, 48, 64])
    b_conv2 = bias_variable([64])
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool(h_conv2, ksize=(2, 1), stride=(2, 1))

    #第三层, 16*64*64 => 8*32*128
    W_conv3 = weight_variable([5, 5, 64, 128])
    b_conv3 = bias_variable([128])
    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)
    h_pool3 = max_pool(h_conv3, ksize=(2, 2), stride=(2, 2))

    return inputs, h_pool3

    #全连接
    W_fc1 = weight_variable([16 * 8 * OUTPUT_SHAPE[1], OUTPUT_SHAPE[1]])
    b_fc1 = bias_variable([OUTPUT_SHAPE[1]])

    conv_layer_flat = tf.reshape(h_pool3, [-1, 16 * 8 * OUTPUT_SHAPE[1]])

    features = tf.nn.relu(tf.matmul(conv_layer_flat, W_fc1) + b_fc1)
    #（batchsize,256）
    shape = tf.shape(features)
    features = tf.reshape(features, [shape[0], OUTPUT_SHAPE[1], 1])  # batchsize * outputshape * 1
    return inputs,features


def convolutional_layers2(inputs):
    # 4 conv layer
    # w_alpha=0.01
    # w_c1 = tf.Variable(w_alpha * tf.random_normal([5, 5, 3, 32]))
    
    w_c1 = tf.get_variable(name='w_c1', shape=[5, 5, 3, 64],
                           initializer=tf.contrib.layers.xavier_initializer())
    b_c1 = tf.Variable(tf.constant(0.1, shape=[64]))
    conv1 = tf.nn.bias_add(tf.nn.conv2d(inputs, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1)
    conv1 = tf.nn.relu(conv1)
    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


    # w_c2 = tf.Variable(w_alpha * tf.random_normal([5, 5, 32, 64]))
    w_c2 = tf.get_variable(name='w_c2', shape=[5, 5, 64, 128],
                           initializer=tf.contrib.layers.xavier_initializer())
    b_c2 = tf.Variable(tf.constant(0.1, shape=[128]))
    conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2)
    conv2 = tf.nn.relu(conv2)
    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


    # w_c3 = tf.Variable(w_alpha * tf.random_normal([3, 3, 64, 64]))
    w_c3 = tf.get_variable(name='w_c3', shape=[3, 3, 128, 128],
                           initializer=tf.contrib.layers.xavier_initializer())
    b_c3 = tf.Variable(tf.constant(0.1, shape=[128]))
    conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3)
    conv3 = tf.nn.relu(conv3)
    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


    # w_c4 = tf.Variable(w_alpha * tf.random_normal([3, 3, 64, 64]))
    w_c4 = tf.get_variable(name='w_c4', shape=[3, 3, 128, 128],
                           initializer=tf.contrib.layers.xavier_initializer())
    b_c4 = tf.Variable(tf.constant(0.1, shape=[128]))
    conv4 = tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, strides=[1, 1, 1, 1], padding='SAME'), b_c4)
    # conv4 = tf.nn.relu(conv4)
    # conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    # conv4 = tf.nn.dropout(conv4, keep_prob)

    return inputs,conv4
    # print(conv4.get_shape().as_list())
    #shape = tf.shape(conv4)
    shape = conv4.get_shape().as_list()
    features = tf.reshape(conv4, [shape[0], OUTPUT_SHAPE[1], 1])  # batchsize * outputshape * 1
    return inputs,features